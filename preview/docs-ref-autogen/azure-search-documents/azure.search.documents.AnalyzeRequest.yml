### YamlMime:UniversalReference
api_name: []
items:
- children: []
  class: azure.search.documents.AnalyzeRequest
  fullName: azure.search.documents.AnalyzeRequest
  inheritance:
  - inheritance:
    - type: builtins.object
    type: msrest.serialization.Model
  langs:
  - python
  module: azure.search.documents
  name: AnalyzeRequest
  summary: 'Specifies some text and analysis components used to break that text into
    tokens.


    All required parameters must be populated in order to send to Azure.'
  syntax:
    parameters:
    - description: Required. The text to break into tokens.
      id: text
      type:
      - str
    - description: 'The name of the analyzer to use to break the given text. If this
        parameter is

        not specified, you must specify a tokenizer instead. The tokenizer and analyzer
        parameters are

        mutually exclusive. Possible values include: ''ar.microsoft'', ''ar.lucene'',
        ''hy.lucene'',

        ''bn.microsoft'', ''eu.lucene'', ''bg.microsoft'', ''bg.lucene'', ''ca.microsoft'',
        ''ca.lucene'', ''zh-

        Hans.microsoft'', ''zh-Hans.lucene'', ''zh-Hant.microsoft'', ''zh-Hant.lucene'',
        ''hr.microsoft'',

        ''cs.microsoft'', ''cs.lucene'', ''da.microsoft'', ''da.lucene'', ''nl.microsoft'',
        ''nl.lucene'',

        ''en.microsoft'', ''en.lucene'', ''et.microsoft'', ''fi.microsoft'', ''fi.lucene'',
        ''fr.microsoft'',

        ''fr.lucene'', ''gl.lucene'', ''de.microsoft'', ''de.lucene'', ''el.microsoft'',
        ''el.lucene'',

        ''gu.microsoft'', ''he.microsoft'', ''hi.microsoft'', ''hi.lucene'', ''hu.microsoft'',
        ''hu.lucene'',

        ''is.microsoft'', ''id.microsoft'', ''id.lucene'', ''ga.lucene'', ''it.microsoft'',
        ''it.lucene'',

        ''ja.microsoft'', ''ja.lucene'', ''kn.microsoft'', ''ko.microsoft'', ''ko.lucene'',
        ''lv.microsoft'',

        ''lv.lucene'', ''lt.microsoft'', ''ml.microsoft'', ''ms.microsoft'', ''mr.microsoft'',
        ''nb.microsoft'',

        ''no.lucene'', ''fa.lucene'', ''pl.microsoft'', ''pl.lucene'', ''pt-BR.microsoft'',
        ''pt-BR.lucene'', ''pt-

        PT.microsoft'', ''pt-PT.lucene'', ''pa.microsoft'', ''ro.microsoft'', ''ro.lucene'',
        ''ru.microsoft'',

        ''ru.lucene'', ''sr-cyrillic.microsoft'', ''sr-latin.microsoft'', ''sk.microsoft'',
        ''sl.microsoft'',

        ''es.microsoft'', ''es.lucene'', ''sv.microsoft'', ''sv.lucene'', ''ta.microsoft'',
        ''te.microsoft'',

        ''th.microsoft'', ''th.lucene'', ''tr.microsoft'', ''tr.lucene'', ''uk.microsoft'',
        ''ur.microsoft'',

        ''vi.microsoft'', ''standard.lucene'', ''standardasciifolding.lucene'', ''keyword'',
        ''pattern'',

        ''simple'', ''stop'', ''whitespace''.'
      id: analyzer
      type:
      - str
      - search_service_client.models.AnalyzerName
    - description: 'The name of the tokenizer to use to break the given text. If this
        parameter

        is not specified, you must specify an analyzer instead. The tokenizer and
        analyzer parameters

        are mutually exclusive. Possible values include: ''classic'', ''edgeNGram'',
        ''keyword_v2'',

        ''letter'', ''lowercase'', ''microsoft_language_tokenizer'', ''microsoft_language_stemming_tokenizer'',

        ''nGram'', ''path_hierarchy_v2'', ''pattern'', ''standard_v2'', ''uax_url_email'',
        ''whitespace''.'
      id: tokenizer
      type:
      - str
      - search_service_client.models.TokenizerName
    - description: 'An optional list of token filters to use when breaking the given
        text.

        This parameter can only be set when using the tokenizer parameter.'
      id: token_filters
      type:
      - list[str
      - search_service_client.models.TokenFilterName]
    - description: 'An optional list of character filters to use when breaking the
        given text.

        This parameter can only be set when using the tokenizer parameter.'
      id: char_filters
      type:
      - list[str]
  type: class
  uid: azure.search.documents.AnalyzeRequest
references:
- fullName: list[str
  name: list[str
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  uid: list[str
- fullName: search_service_client.models.TokenFilterName]
  name: TokenFilterName]
  spec.python:
  - fullName: search_service_client.models.TokenFilterName
    name: TokenFilterName
    uid: search_service_client.models.TokenFilterName
  - fullName: ']'
    name: ']'
  uid: search_service_client.models.TokenFilterName]
- fullName: list[str]
  name: list[str]
  spec.python:
  - fullName: list
    name: list
    uid: list
  - fullName: '['
    name: '['
  - fullName: str
    name: str
    uid: str
  - fullName: ']'
    name: ']'
  uid: list[str]
